{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e122ad-34bb-4d0a-a749-0373d00f20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import relevant functionality\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# from langchain_tavily import TavilySearch\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# # Create the agent\n",
    "# memory = MemorySaver()\n",
    "# model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n",
    "# search = TavilySearch(max_results=2)\n",
    "# tools = [search]\n",
    "# agent_executor = create_react_agent(model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbeb5cd0-3c62-4c5f-bf56-68027653a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46028034-1ec7-45c8-bd34-b462f6e79cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
    "\n",
    "## make model agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f4053b-4bb5-465c-8785-61839b41fb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today? üòä'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi!\"\n",
    "response = model.invoke([{\"role\": \"user\", \"content\": query}])\n",
    "response.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6493632-14d9-4078-9fdc-a96f2441f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [multiply, divide]\n",
    "\n",
    "## add tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98970e8-d8f0-4d0d-bcf9-27fda89691d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055ad631-82ee-4c0a-b0fa-47eed31f1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message content: \n",
      "\n",
      "Tool calls: [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_y84BGR6Q2BBsR2q9h5SSFacY', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the product of 2 times 3\"\n",
    "response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])\n",
    "\n",
    "print(f\"Message content: {response.text()}\\n\")\n",
    "print(f\"Tool calls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75c2137-121b-4d0b-afcd-947cfdb89ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(model, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b375e486-bee9-4139-aa10-99f3a0582844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the product of 2 times 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_CK1nYXcBsC2NNe60u8YMVtM6)\n",
      " Call ID: call_CK1nYXcBsC2NNe60u8YMVtM6\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The product of 2 times 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the product of 2 times 3\"\n",
    "input_message = {\"role\": \"user\", \"content\": query}\n",
    "response = agent_executor.invoke({\"messages\": [input_message]})\n",
    "\n",
    "## Show results all at once\n",
    "# for message in response[\"messages\"]:\n",
    "#     message.pretty_print()\n",
    "\n",
    "# Show immediate message\n",
    "for step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ef9070-a6d1-426c-962d-13880da760f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63bd31b-c68f-4da2-846e-349eec5efb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you get the product of 115 by 5? Then, divide the answer by 10. Who owns the dog?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_zKCAxeewUwS71WVQRKjsTH0k)\n",
      " Call ID: call_zKCAxeewUwS71WVQRKjsTH0k\n",
      "  Args:\n",
      "    a: 115\n",
      "    b: 5\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "575\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (call_Zm9x9PqeFhzZrrQr7TuBnWaU)\n",
      " Call ID: call_Zm9x9PqeFhzZrrQr7TuBnWaU\n",
      "  Args:\n",
      "    a: 575\n",
      "    b: 10\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "57.5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The product of 115 by 5 is 575. When you divide 575 by 10, you get 57.5. As for \"who owns the dog,\" that information hasn't been specified‚Äîplease provide more context if you need help with that!\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you get the product of 115 by 5? Then, divide the answer by 10. Who owns the dog?\"\n",
    "\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [(\"user\", query)]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f505459e-346f-45d6-8fa4-64cce46b4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# system_prompt = \"\"\"\n",
    "# You are operations person with extensive years of experience. Provide analysis in detail. Do not make up any answers. Say I don't know if you do not have the answers.\n",
    "\n",
    "# Always think of production thoughput is king on decisions!\n",
    "# \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an electrical engineer with extensive years of experience. \n",
    "Ensure safety and reliability is maintained on decisions!\n",
    "\n",
    "Avoid using numbered bullets or list. Answer in concise paragraphs and thorough response. \n",
    "Provide analysis in detail. Do not make up any answers. Say I don't know if you do not have the answers.\n",
    "\"\"\"\n",
    "\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory, prompt=system_prompt)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aabf4a6a-9504-4408-b4b5-5e4eb6c1753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I have a AG mill. Do you recommend to push it hard to gain production increase?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Increasing throughput in an AG (Autogenous Grinding) mill to boost production is tempting, but the decision requires careful analysis. AG mills have distinct operational limits governed by mill motor power, mechanical integrity, liner/lifter wear, ore characteristics, and downstream process capacity.\n",
      "\n",
      "Pushing an AG mill ‚Äúhard‚Äù can indeed lead to higher short-term production, but if you approach or exceed motor power availability, you risk tripping the drive or overheating components, which can cause long downtimes. Overloading can also result in excessive wear on linings and lifters, unplanned maintenance, and possible shell or trunnion cracks. Throughput increases that do not match downstream capability can create bottlenecks or overload other circuits like flotation or thickening.\n",
      "\n",
      "The ore‚Äôs hardness and variability play a crucial role. If the ore becomes harder, the power draw and mill load increase‚Äîyou could reach a point where the mill begins to ‚Äúbog‚Äù (i.e., stop grinding efficiently), leading to underperformance or even equipment damage. Furthermore, elevated material flow can decrease grinding efficiency, leading to higher recirculating loads or coarser grind, impacting metallurgical recovery.\n",
      "\n",
      "From an electrical engineering perspective, consistently operating close to, or above, the mill‚Äôs rated power can accelerate failures in the drive system, especially if thermal management is marginal. Continuously running at higher loads will result in higher current through the motor, increased heat generation, and faster insulation aging, affecting reliability.\n",
      "\n",
      "Reliability and safety must come before short-term gains. I recommend a detailed production and reliability review. Analyze historical power usage, downtime records, liner/lifter wear rates, and any past incidences of electrical issues. Also, review the maximum design limits stated by the equipment supplier. Consider increasing throughput incrementally, with careful monitoring of mill load, power draw, product size, and indicators of mechanical or electrical stress. Coordination with process metallurgy and maintenance teams is essential to balancing production targets with plant health.\n",
      "\n",
      "Simply pushing the AG mill hard without comprehensive analysis and safeguards can result in costly failures and reduced overall performance in the long term.\n"
     ]
    }
   ],
   "source": [
    "query = \"I have a AG mill. Do you recommend to push it hard to gain production increase?\"\n",
    "\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [(\"user\", query)]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb1dd4-d3ff-4358-9f97-e52cf32e3e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9fdfc-53dc-492d-a65d-e3536bc64133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_agent(task: str, model: str = \"gpt-5\"):\n",
    "    \"\"\"\n",
    "    Ejecuta una tarea de investigaci√≥n usando herramientas con aisuite (sin bucle manual).\n",
    "    \"\"\"\n",
    "    print(\"==================================\")\n",
    "    print(\"üîç Research Agent\")\n",
    "    print(\"==================================\")\n",
    "    max_tool_call = 3\n",
    "    tool_calls = 0\n",
    "    prompt = f\"\"\"\n",
    "You are a research assistant with access to the following tools:\n",
    "- arxiv_tool: for finding academic papers\n",
    "- tavily_tool: for general web search\n",
    "- wikipedia_tool: for encyclopedic knowledge\n",
    "\n",
    "Task:\n",
    "{task}\n",
    "\n",
    "Today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "Limit tool calling into {max_tool_call} maximum\n",
    "\"\"\"\n",
    "    print(prompt)\n",
    "    def run_tool(name, args):\n",
    "        if name == \"arxiv_search_tool\":\n",
    "            return research_tools.arxiv_search_tool(**args)\n",
    "        if name == \"tavily_search_tool\":\n",
    "            return research_tools.tavily_search_tool(**args)\n",
    "        if name == \"wikipedia_search_tool\":\n",
    "            return research_tools.wikipedia_search_tool(**args)\n",
    "        return {\"error\": f\"Unknown tool: {name}\"}\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\":prompt.strip()}]\n",
    "    tools = [research_tools.arxiv_tool_def, research_tools.tavily_tool_def, research_tools.wikipedia_tool_def]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "\n",
    "        msg = response.choices[0].message\n",
    "\n",
    "        messages.append(msg)\n",
    "        # kept = msg.tool_calls[:max_tool_call]\n",
    "        # sanitized_assistant = {\n",
    "        #     \"role\": \"assistant\",\n",
    "        #     \"content\": msg.content,\n",
    "        #     \"tool_calls\": kept,\n",
    "        # }\n",
    "        # messages.append(sanitized_assistant)\n",
    "        for call in msg.tool_calls:\n",
    "            tool_calls += 1\n",
    "            print(call.function.name, call.function.arguments)\n",
    "            if tool_calls <= max_tool_call:\n",
    "                result = run_tool(call.function.name, json.loads(call.function.arguments))\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": call.id,\n",
    "                    \"name\": call.function.name,\n",
    "                    \"content\": json.dumps(result)\n",
    "                })\n",
    "        # if tool_calls > 3:\n",
    "        #     messages.append({\n",
    "        #         \"role\": \"assistant\",\n",
    "        #         \"content\": \"I have reached the maximum tool usage as instructed. \",\n",
    "        #     })\n",
    "            \n",
    "        final_response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            reasoning_effort = \"minimal\"\n",
    "        )\n",
    "        \n",
    "        content = final_response.choices[0].message.content\n",
    "        print(\"‚úÖ Output:\\n\", content)\n",
    "        used_tokens = response.usage.total_tokens\n",
    "        print(\"Used Tokens:\\n\", used_tokens)\n",
    "        return content, used_tokens\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n",
    "        return f\"[Model Error: {str(e)}]\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
